---
title: "Multiscale space-time regression with `stgam`"
author: "Lex Comber, Paul Harris and Chrs Brunsdon"
date: "June 2024"
output: rmarkdown::html_vignette
bibliography: vignette.bib
header-includes:
   - \usepackage{caption}

vignette: >
  %\VignetteIndexEntry{Introduction to space-time GAMS with `stgam`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---

\captionsetup{width=5in}

```{r, include = FALSE}
library(knitr)
# library(kableExtra)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  options(width = 100)
)
```

## Overview: GAMs with TP smooths 

The aim of this vignette is provide an introduction to the `stgam` package and to demonstrate how it constructs space-time varying coefficient models. It revises previous approaches to GAM-based space-time modelling by replacing Gaussian Process (GP) smooths [@stgam_v0; @comber2023multiscale] (which are fine for spatial models but not space-time ones) with Tensor Product (TP) smooths with GP bases which are equally suited to spatial models and space-time models. Here, the concept of a Gaussian Process [@Wood2020] is important in the context of regression modelling. It provides a *data model* of the likelihood that a given data set is generated given a statistical model involving some unknown parameters and in regression modelling, the unknown parameters are the regression parameters. These are described formally in @comber2024multiscale. 

The key ideas underpinning the development of varying coefficient models with GAMs in the `stgam` package and this vignette are:

1. A focus on process understanding - how processes vary in space and time - not prediction. 
1. The erroneous assumption of stationarity in standard linear regression. 
1. Many geographic processes have a Gaussian form when they are examined over 2-dimensional space (distance decay) or time (temporal decay). 
1. GAMs can include different forms of smooths (splines) including Gaussian Processes. 
1. A GAM with smooths parameterised by location defines a spatially varying coefficient (SVC) model.
1. This can be extended to space-time.

Details of the need for varying coefficient models and the evolution of their application from spatial to space-time coefficient modelling can be found in @comber2024multiscale and @comber2025st. 

The critical consideration when working with space-time GAMs is the nature of the smooth. Previous work used Gaussian Process smooths for the spatial case [@comber2024multiscale] but this form is not appropriate for the space-time as distances in space and time are not *isotropic* (i.e. equivalent in *all* dimensions). Tensor Product (TP) smooths allow spatio-temporal terms to be fitted via a marginal spatial smooth and a marginal time smooth, both of which can be specified with a GP basis.

TP smooths require a length scale or range parameter, $\rho$ to be specified, for both the spatial and temporal dimensions for each predictor variable. These define the distance at which the correlation function falls below some small value and thereby the space-time dependencies between the target and the predictor variable (i.e. the scales of their interaction). As `stgam` provides a space-time wrapper to functions in the `mgcv` R package [@mgcv], $\rho_{space}$ and $\rho_{time}$  are optimised by minimising an un-biased risk estimator the model, the Generalized Cross-Validation (GCV) score. This is recommended for `mgcv` GAM comparison GAMs [@wood2017generalized] and replaced BIC in earlier versions of the `stgam` package.

The optimisation of the $\rho_{space}$ and $\rho_{time}$ parameters for each predictor variable provides a direct indication of the scales of the individual target-to-predictor-variable interactions, in a similar as bandwidths in multiscale GWR [@yang2014extension; @fotheringham2017multiscale], but extending into the temporal dimension. The spatial and temporal dependencies are directly indicated through the $\rho_{space}$ and $\rho_{time}$ parameters.

A second and potentially important consideration is to determine model form (model specification), and *how* the space-time target-to-predictor-variable interactions are specified, for each predictor variable. The aim is to avoid making unsupported assumptions about the presence and nature of any space-time interaction in the data and thus in the models. This is done by creating multiple models, with different space-time forms for each predictor variable, and evaluating them using GVC.

The Part 1 of this vignette, provides a walk through of the 2 stage analysis process using `mgcv` functions to create a SVC and then a STVC. Part 2 undertakes the same STVC analysis, but this time with `stgam` wrapper functions. The advice is to work through Part 1 (30 minutes) before Part 2, to understand how the `stgam` workflows approach multiscale spatially and temporally varying coefficient modelling. 


## Part 1: A walk through example

### Set up: data, packages and model objectives

You should install the `stgam` package either from CRAN or from GitHub:

```{r eval = F}
install.packages("stgam", dependencies = TRUE)
remotes::install_github("lexcomber/stgam")
```

And then make sure the required packages and data are loaded:

```{r, warning=F, message=F}
# load the packages
library(stgam)
library(cols4all)   # for nice shading in graphs and maps
library(cowplot)    # for managing plots
library(dplyr)      # for data manipulation 
library(ggplot2)    # for plotting and mapping
library(glue)       # for model construction 
library(mgcv)       # for GAMs
library(sf)         # for spatial data
library(doParallel) # for parallelising operations
library(purrr)      # for model construction
library(tidyr)      # for model construction 
# load the data 
data(productivity)
data(us_data) 
```

The `productivity` data is annual economic productivity data for the 48 contiguous US states (with Washington DC merged into Maryland), for years 1970 to 1985 (16 years). This was extracted from the `plm` package [@croissant2022plm]. The `us_data` is a spatial dataset of the US states in a USA Contiguous Equidistant Conic projection (ESRI:102005) from the `spData` package [@bivand2019spdata]. The `productivity` data includes locational information of the state geometric centroid  in the same projection.  The code below maps the `X` and `Y` locations in `productivity` along with the US state areas.

```{r locationplot, message = F, warning=F, fig.height = 4, fig.width = 7, fig.cap = "The US States and the geoemtric centroids used as locations."}
ggplot() +  geom_sf(data = us_data, fill = NA) +
  geom_point(data = productivity |> filter(year == "1975"), aes(x = X, y = Y)) +
  theme_bw() + ylab("") + xlab("")
```

The data attributes can be examined:
```{r}
head(productivity)
```
The data can be used to construct regression models with Private capital stock (`privC`) as the target variable, and Unemployment (`unemp`) and Public capital (`pubC`) as the predictor variables, where the coefficient functions are assumed to be realisation of a Gaussian Process (GP) introduced above. The objectives are to construct a spatially varying coefficient model (SVC), and then a space-time varying coefficient model (STVC). 


### A simple SVC 

#### Data 

A spatially varying coefficient model of `privC` will be created using the `productivity` dataset, using the data for 1975, this is extracted using the code below which also defines an intercept column (`Intercept`) to allow it be treated as an addressable term in the model. 
```{r}
input_data <- 
  productivity |> 
  filter(year == 1975) |> 
  mutate(Intercept = 1)
```

#### Optimising length scale parameters

Next the length scale parameter, $\rho_{space}$, is determined for each predictor variable. This specifies the distance at which the spatial dependencies are zero in the data. The code below determines the maximum distance between the observations (US states in this case), defines a function that constructs a GAM using TP smooths with GP bases and returns their GCV unbiased risk estimator score, and then uses the `optimise` R function to determine optimal $\rho_{space}$ value. Note, that it is important that the spatial data are in projected coordinates rather than in degrees. You can use the `st_transform` function the `sf` package to do this.

```{r}
# max distance as an upper limit to explore
d_max <- 
  input_data |>
  transmute(X = X, Y = Y) |> 
  dist() |> as.vector() |> ceiling() |> max()
# function to optimise - gets gcv score 
get_XY_gcv = function(x, var) {
  f = as.formula(paste0("privC ~ te(X, Y, d = 2, bs = 'gp', by =", var, ", m = list(c(3, x)))") )
  gam.i = gam(f, data = input_data, method = "GCV.Cp")
  return(as.vector(gam.i$gcv.ubre))
}  
rho_sp_int = optimise(get_XY_gcv, c(0,d_max), var = "Intercept", maximum=FALSE)$minimum
rho_sp_unemp = optimise(get_XY_gcv, c(0,d_max), var = "unemp", maximum=FALSE)$minimum
rho_sp_pubc = optimise(get_XY_gcv, c(0,d_max), var = "pubC", maximum=FALSE)$minimum
```

Note the way that formula, `f`, is specified in the `get_XY_gcv()` function above: 

- the tensor product smooth (`te()`) is specified;
- it is parameterised with observation location (`X` and `Y`); 
- and with a Gaussian Process marginal basis (`bs = 'gp'`); 
- the dimensions of the marginal basis, `d` is specified as 2 dimensional basis for location (`X` and `Y`); 
- the smooth is specified with a third-order smooth ( `m = list(c3, rho))` ) for a Mat√©rn correlation function with $\kappa = 1.5$ (see @comber2025st for a discussion of this). 

The distances can be examined to show the spatial dependencies between the target variable and the predictor variables (see also the Table below):
```{r eval = F}
# distances in km
c(rho_sp_int, rho_sp_unemp, rho_sp_pubc)/1000
```

```{r tab_rho1, echo = F, eval = T}
df = data.frame(Variable = c("Intercept", "Unemployent", "Public Capital"), 
                rho = c(rho_sp_int, rho_sp_unemp, rho_sp_pubc)/1000)
# table
df |>
  kable(booktabs = T, row.names = F, linesep = "", 
        # format = "latex",
        col.names = c("Variable", "$\\rho_{space}$"),
        digits = 2,
	      caption = paste0("\\label{tab:tab_rho1}The optimised spatial length scales (in km)."))  
```

#### Determining model form

Having determined the correlation length scales the model form can be determined. For each predictor variable there are 3 options:

i. It is omitted.
ii. It is included as a parametric response with no TP smooth.
iii. It is included in a TP smooth with location.

The intercept can be treated similarly, but without it being absent (i.e. 2 options).

To evaluate these, the code below first defines a grid of numbers for each of the options above, for each predicted variable. This is passed to a function to create the formula specifying each model, with different terms and smooths, which in turn is passed to the `gam` function. 


```{r eval = T}
# define grid of combinations (nrow = 18)
terms_gr = expand.grid(Intercept = 1:2, unemp = 1:3, pubC = 1:3) 
# examine a random slice
terms_gr |> slice_sample(n = 6)
```

A function is defined to create the equations: here this is bespoke to the covariate names in `input_data`  and the numbers in `terms_gr`:
```{r}
# define a function to make the equations
makeform_svc <- function(intcp = 1, unemp, pubC,  
                          rho_space_int, rho_space_unemp, rho_space_pubc,
                          bs='gp') {
  intx <- c("",
            glue("+te(X,Y,d=2,m=list(c(3,{rho_space_int})),bs='{bs}',by=Intercept)"))[intcp]
  unempx <- c("", 
              "+ unemp",
             glue("+te(X,Y,d=2,m=list(c(3,{rho_space_unemp})),bs='{bs}',by=unemp)"))[unemp]
  pubCx <- c("", 
             "+ pubC",
             glue("+te(X,Y,d=2,m=list(c(3,{rho_space_pubc})),bs='{bs}',by=pubC)"))[pubC]
  return(formula(glue("privC~Intercept-1{unempx}{intx}{pubCx}")))
}
```
You could check what this does
```{r eval = F}
makeform_svc(1,3,2,rho_sp_int, rho_sp_unemp, rho_sp_pubc)
```

A function to undertake the analysis and record the GCV score, return the indices and the formula is defined below. Note that this has the `terms_gr` object defined above embedded in it, taking just an index of the grid row number as input, and returning the GCV value for each model:
```{r}
do_gam = function(i){
  	f <- makeform_svc(intcp = terms_gr$Intercept[i],
  	                 unemp = terms_gr$unemp[i],
  	                 pubC = terms_gr$pubC[i],
  	                 rho_space_int = rho_sp_int,
  	                 rho_space_unemp = rho_sp_unemp,
  	                 rho_space_pubc = rho_sp_pubc,
  	                 bs='gp')
  	m = gam(f,data=input_data)
    gcv = m$gcv.ubre
    index = data.frame(Intercept = terms_gr$Intercept[i],
                       unemp = terms_gr$unemp[i],
                       pubC = terms_gr$pubC[i])
    f = paste0('privC~', as.character(f)[3] )			
    return(data.frame(index, gcv, f))
}
```
This can be tested: 
```{r eval = F}
terms_gr[10,]
do_gam(10)
```

Finally, this can be put in a loop to evaluate all of the potential models:
```{r dogam, eval = T}
t1 = Sys.time()
res_gam <- NULL 
for(i in 1:nrow(terms_gr)) {
  res.i = do_gam(i)
  res_gam = rbind(res_gam, res.i)
}
Sys.time() - t1
```
Note that, for more complex problems, the loop above can be parallelised. 

Having evaluated multiple models and recorded the GCV values for them, the models can be ranked and the best one selected: 
```{r}
# sort the results
mod_comp <- tibble(
    res_gam) |>
    rename(GCV = gcv) |>
    arrange(GCV) 
# transpose the indices to model terms 
# rank and return the top 10 results
int_terms <- function(x) c("Fixed", "te_S")[x]
var_terms <- function(x) c("---", "Fixed", "te_S")[x]
mod_comp_tab <- 
  mod_comp |> 
  slice_head(n = 10) |> 
  mutate(across(unemp:pubC,var_terms)) |>
  mutate(Intercept = int_terms(Intercept)) |>
  rename(`Unemployment` = unemp,
         `Public Captial` = pubC) |>
  mutate(Rank = 1:n()) |>
  relocate(Rank) 
```
The GCV ordered results can be examined: 
```{r eval = T}
mod_comp_tab |> select(-f)
```
The results are sorted by GCV and suggest that the best model is one in which the Intercept and Unemployment predictor variables are fixed, parametric terms, and Public Capital is included in spatial TP smooth. Interestingly the top 6 models all include Public Capital in spatial TP smooth. 

#### The final model 

The final SVC model can be constructed by extracting the formula from the top ranked model in `mod_comp_tab`:

```{r}
# extract and examine the formula
f = as.formula(mod_comp_tab$f[1])
f
# create the final model
svc.gam = gam(f, data = input_data)
```
Here, for the `pubC` predictor variable, a TP smooth with a BP basis is specified for `X` and `Y` (the coordinates in geographic space) and the covariate is included via the `by` parameter. 

The model output can be assessed using the `gam.check` function in the`mgcv` package.Here the `k'` and  `edf` parameters are not close, and the `k-index` is greater than 1, so this model is well tuned. The diagnostic plots are again generated by the `gam.check` function as below:

```{r ch2gamcheck, fig.height = 7, fig.width = 7, fig.cap = "The GAM GP SVC diagnostic plots."}
# check 
gam.check(svc.gam)
```

The model summary can also be examined:
```{r}
# model summary
summary(svc.gam)
```

So overall, it can be seen that:

1. The model is well tuned: the all effective degrees of freedom (`edf`) for the `pubC` smooth  are well below `k` in the `gam.check()` printed output.
1. All of the the fixed parametric terms are significant.
1. The smooth terms for `pubC` is locally significant and spatially varying.

#### The varying coefficients

The spatially varying coefficient estimates can be extracted using `predict`. To do this a dummy data set is created with the `pubC` term set to 1, and the intersect and `unemp` terms set to zero. The result is that the predicted values for the coefficient estimate are just a function of $\beta_2$, the `pubC` coefficient estimate at each location.

```{r}
get_b2<- productivity |> filter(year == "1975") |> mutate(Intercept = 0, unemp = 0, pubC = 1)
svc <- productivity |> filter(year == "1975") |> 
  mutate(b_pubC = predict(svc.gam, newdata = get_b2))
```

The resulting `data.frame` called `svc` has a new variable called `b_pubC` which is the spatially varying coefficient estimate for `pubC`. For comparison, we can generate the coefficient estimates for the Intercept and `unemp` in the same way by setting the other terms in the model to zero: 

```{r}
get_b0 <- productivity |> filter(year == "1970") |> mutate(Intercept = 1, unemp = 0, pubC = 0)
svc <- svc |> mutate(b_Intercept = predict(svc.gam, newdata = get_b0))
get_b1 <- productivity |> filter(year == "1970") |> mutate(Intercept = 0, unemp = 1, pubC = 0)
svc <- svc |> mutate(b_unemp = predict(svc.gam, newdata = get_b1))
```

So `svc` has the records for the year 1975 and three new columns for `b_Intercept`, `b_unemp` and `b_pubC` The distribution of the spatially varying coefficient estimates can be examined, and unsurprisingly only that for `b_pubC` is spatially varying:

```{r eval = T}
svc |> select(b_Intercept, b_unemp, b_pubC) |>
  apply(2, summary) |> round(4)
```

Standard `dplyr` and `ggplot` approaches can be used to join and map the coefficient estimates as in the figure below. Notice the strong spatial trend of the relationship between the target variable and Public capital (`pubC`), with particularity high coefficient estimates in the south, suggesting in these areas much greater return in private capital as a result of public capital investments (in 1975!). 

```{r ch2svccoefs, fig.width = 7, fig.cap = "The spatially varying coefficient (SVC) estimates."}
# join the data 
svc_sf <-
  us_data |> left_join(svc |> select(GEOID, b_Intercept, b_unemp, b_pubC), by = "GEOID")
# plot the insignificant coefficient estimates
tit =expression(paste(""*beta[pubC]*""))
ggplot(data = svc_sf, aes(fill=b_pubC)) + 
  geom_sf() + 
  scale_fill_continuous_c4a_div(palette="brewer.spectral",name=tit) + 
  coord_sf() +
  ggtitle("Public captial: significant")
```

### A simple STVC


#### Data 

The `productivity` data was filtered for 1975 the the SVC above in which the `X-Y` location of the 48 states was used to parameterise the TP smooths. The same structure can be used to create a temporally vary coefficient model (STVC), with smooths now extended to include the `year` parameter either singly or with location (`X` and `Y`), but this time not restricting the analysis data to records from a single year.

```{r}
# redefine the input data
input_data <- 
  productivity |> 
  mutate(Intercept = 1)
```

#### Optimising length scale parameters

Again the first step is to determine the length scale parameters, $\rho_{space}$ and $\rho_{time}$, for each predictor variable (**NB** new $\rho_{space}$ values are needed because the input data has changed.) The code below does this:  
```{r}
# space: max distance as before
d_max <- 
  input_data |>
  transmute(X = X, Y = Y) |> 
  dist() |> as.vector() |> ceiling() |> max()
# use the same get_XY_gcv function as before
rho_sp_int = optimise(get_XY_gcv, c(0,d_max), var = "Intercept", maximum=FALSE)$minimum
rho_sp_unemp = optimise(get_XY_gcv, c(0,d_max), var = "unemp", maximum=FALSE)$minimum
rho_sp_pubc = optimise(get_XY_gcv, c(0,d_max), var = "pubC", maximum=FALSE)$minimum
# time:  as an upper limit to explore
t_max <- 
  input_data |>
  transmute(T = year) |> 
  as.vector() |> range() |> diff() 
# function to optimise - gets gcv score 
get_time_gcv = function(x, var) {
  f = as.formula(paste0("privC ~ te(year, d = 1, bs = 'gp', by =", var, ", m = list(c(3, x)))") )
  gam.i = gam(f, data = input_data, method = "GCV.Cp")
  return(as.vector(gam.i$gcv.ubre))
}
rho_t_int = optimise(get_time_gcv, c(0,t_max), var = "Intercept", maximum=FALSE)$minimum
rho_t_unemp = optimise(get_time_gcv, c(0,t_max), var = "unemp", maximum=FALSE)$minimum
rho_t_pubc = optimise(get_time_gcv, c(0,t_max), var = "pubC", maximum=FALSE)$minimum
```


Again, note the way that formula, `f`, is specified in the `get_time_gcv()` function above: 

- the tensor product smooth (`te()`) is specified;
- it is parameterised with observation time (`year`); 
- and with a Gaussian Process marginal basis (`bs = 'gp'`); 
- the dimensions of the marginal basis, `d` is specified as 1 dimensional basis for time (`year`); 
- the temporal smooth is again specified with a third-order smooth ( `m = list(c3, rho))` ) for a Mat√©rn correlation function with $\kappa = 1.5$. 

The length scales can be examined to show the individual spatial and temporal dependencies between the target variable and the predictor variables:
```{r eval = F}
# distances in km 
c(rho_sp_int, rho_sp_unemp, rho_sp_pubc)/1000
# distances in years
c(rho_t_int, rho_t_unemp, rho_t_pubc)
```


```{r tab_rho2, echo = F, eval = T}
df = data.frame(Variable = c("Intercept", "Unemployent", "Public Capital"), 
                rho_space = c(rho_sp_int, rho_sp_unemp, rho_sp_pubc)/1000,
                rho_time = c(rho_t_int, rho_t_unemp, rho_t_pubc))
# table
df |>
  kable(booktabs = T, row.names = F, linesep = "", 
        # format = "latex",
        col.names = c("Variable", "$\\rho_{space}$", "$\\rho_{time}$"),
        digits = 2,
	      caption = paste0("\\label{tab:tab_rho2}The optimised spatial and temporal length scales, in km and years)."))  
```

#### Determining model form

Having determined the spatial and temporal correlation length scales the model form can be determined. For each predictor variable there are now 6 options (the first 3 as before):

i. It is omitted.
ii. It is included as a parametric response with no TP smooth.
iii. It is included in a TP smooth with location.
iv. It is included a TP smooth with time.
v. It is included in a single TP smooth with location and time.
vi. It is included in 2 separate TP smooths with location and time.

The intercept can be treated similarly, but without it being absent (i.e. 5 options). The `makeform_svc` function defined above is extended to accommodate these additional options:
```{r}
makeform_stvc <- function(intcp = 1, 
                          unemp = 5, 
                          pubC = 1,  
                          rho_space_int, rho_space_unemp, rho_space_pubc,
                          rho_time_int, rho_time_unemp, rho_time_pubc,
                          bs='gp') {
  intx <- c("",
            glue("+te(X,Y,d=2,m=list(c(3,{rho_space_int})),bs='{bs}',by=Intercept)"),
            glue("+te(year,d=1,m=list(c(3,{rho_time_int})),bs='{bs}',by=Intercept)"),
            glue("+te(X,Y,year,d=c(2,1),m=list(c(3,{rho_space_int}), c(3,{rho_time_int})),bs='{bs}',by=Intercept)"),
            glue("+te(X,Y,d=2,m=list(c(3,{rho_space_int})),bs='{bs}',by=Intercept) + te(year,d=1,m=list(c(3,{rho_time_int})),bs='{bs}',by=Intercept)"))[intcp]
            
  unempx <- c("", 
              "+ unemp",
             glue("+te(X,Y,d=2,m=list(c(3,{rho_space_unemp})),bs='{bs}',by=unemp)"),
             glue("+te(year,d=1,m=list(c(3,{rho_time_unemp})),bs='{bs}',by=unemp)"),
             glue("+te(X,Y,year,d=c(2,1),m=list(c(3,{rho_space_unemp}),c(3,{rho_time_unemp})),bs='{bs}',by=unemp)"),
             glue("+te(X,Y,d=2,m=list(c(3,{rho_space_unemp})),bs='{bs}',by=unemp) + te(year,d=1,m=list(c(3,{rho_time_unemp})),bs='{bs}',by=unemp)"))[unemp]
  
  pubCx <- c("", 
             "+ pubC",
             glue("+te(X,Y,d=2,m=list(c(3,{rho_space_pubc})),bs='{bs}',by=pubC)"),
             glue("+te(year,d=1,m=list(c(3,{rho_time_pubc})),bs='{bs}',by=pubC)"),
             glue("+te(X,Y,year,d=c(2,1),m=list(c(3,{rho_space_pubc}),c(3,{rho_time_pubc})),bs='{bs}',by=pubC)"),
             glue("+te(X,Y,d=2,m=list(c(3,{rho_space_pubc})),bs='{bs}',by=pubC) + te(year,d=1,m=list(c(3,{rho_time_pubc})),bs='{bs}',by=pubC)"))[pubC]
  
  return(formula(glue("privC~Intercept-1{unempx}{intx}{pubCx}")))
}
```
As before, a grid of index numbers is created for each of the options in the `makeform_stvc` function, which is passed to a redefined `do_gam` function that creates the formula for each model with different terms and smooths, which is passed to the `gam` function, and the GCV value returned.  

```{r}
# define grid of combinations (nrow = 180)
terms_gr = expand.grid(Intercept = 1:5, unemp = 1:6, pubC = 1:6) 
# examine a random slice
terms_gr |> slice_sample(n = 6)
# revised space-time function
do_gam = function(i){
  	f <- makeform_stvc(intcp = terms_gr$Intercept[i],
  	                 unemp = terms_gr$unemp[i],
  	                 pubC = terms_gr$pubC[i],
  	                 rho_space_int = rho_sp_int,
  	                 rho_space_unemp = rho_sp_unemp,
  	                 rho_space_pubc = rho_sp_pubc,
  	                 rho_time_int = rho_t_int,
  	                 rho_time_unemp = rho_t_unemp,
  	                 rho_time_pubc = rho_t_pubc,
  	                 bs='gp')
  	m = gam(f,data=input_data)
    gcv = m$gcv.ubre
    index = data.frame(Intercept = terms_gr$Intercept[i],
                       unemp = terms_gr$unemp[i],
                       pubC = terms_gr$pubC[i])
    f = paste0('privC~', as.character(f)[3] )			
    return(data.frame(index, gcv, f))
}
```
This can be tested: 
```{r eval = F}
terms_gr[21,]
do_gam(21)
```

It can be put in a loop to evaluate all of the potential space-time models - this may take a few minutes to run:
```{r dogam2, eval = F, cache = T}
res_gam2 <- NULL 
for(i in 1:nrow(terms_gr)) {
  res.i = do_gam(i)
  res_gam2 = rbind(res_gam2, res.i)
}
```

```{r echo = F, eval = T}
# save(res_gam2, file = "res_gam2.RData")
load("res_gam2.RData")
```

The best models can be ranked by their GCV values: 
```{r}
# sort the results
mod_comp <- tibble(
    res_gam2) |>
    rename(GCV = gcv) |>
    arrange(GCV) 
# transpose the indices to model terms 
# rank and return the top 10 results
int_terms <- function(x) c("Fixed", "te_S", "te_T", "te_ST", "te_S + te_T")[x]
var_terms <- function(x) c("---", "Fixed", "te_S", "te_T", "te_ST", "te_S + te_T")[x]
mod_comp_tab <- 
  mod_comp |> 
  slice_head(n = 10) |> 
  mutate(across(unemp:pubC,var_terms)) |>
  mutate(Intercept = int_terms(Intercept)) |>
  rename(`Unemployment` = unemp,
         `Public Captial` = pubC) |>
  mutate(Rank = 1:n()) |>
  relocate(Rank) 
```
The GCV ordered results can again be examined: 
```{r eval = T}
mod_comp_tab |> select(-f)
```
The results are sorted by GCV and suggest that the best model is one in which the Intercept and Unemployment are included in temporal TP smooths and Public Capital is included in space-time TP smooth. Interestingly the top 10 models all include Public Capital in space-time TP smooth, with different combinations of temporal TP smooths and fixed parametric forms for the other predictor variables.    

#### The final model 

The final STVC model can be constructed by extracting the formula from the top ranked model in `mod_comp_tab`:

```{r}
# extract the formula
f = as.formula(mod_comp_tab$f[1])
f
# the final model
stvc.gam = gam(f, data = input_data)
```

The model output can be assessed using the `gam.check()` function. The model is well tuned  with large differences between the `k'` and  `edf` parameters, and the `k-index` values are greater than 1 for each smooth, so again this is well tuned model. The diagnostic plots are again generated by the `gam.check` function as below:

```{r echo = T, eval = F}
# check 
gam.check(stvc.gam)
```

The model summary indicates that all of the smooths are significant.

```{r echo = T, eval = F}
# model summary
summary(stvc.gam)
```


#### The varying coefficients

The coefficients of the spatial and temporally vary coefficients can be extracted in the same way as before, but this time using more generic code that underpins the  `calculate_vcs()` function in the `stgam` package:
```{r}
terms = c("Intercept", "unemp", "pubC")
model = stvc.gam
# extract the number of terms
n_t = length(terms)
# create data copies for use in the loop and outputs
input_data_copy = input_data
output_data = input_data
# for each term
for (i in 1:n_t) {
  # create a matrix / data table of terms by observations
  zeros = rep(0, n_t)
  zeros[i] = 1
  terms_df = data.frame(matrix(rep(zeros, nrow(input_data)), ncol = n_t, byrow = T))
  names(terms_df) = terms
  # replace terms in data with 0s /1s
  input_data_copy[, terms] = terms_df
  # calculate standard error and beta
  se.j = predict(model, se = TRUE, newdata = input_data_copy)$se.fit
  b.j = predict(model, newdata = input_data_copy)
  #  create output variables 
  expr1 = paste0("b_", terms[i])
  expr2 = paste0("se_", terms[i])
  # assign to output
  output_data[[expr1]] <- as.vector(unlist(with(output_data, b.j)))
  output_data[[expr2]] <- as.vector(unlist(with(output_data, se.j)))
}
# add predicted y
output_data$yhat = predict(model, newdata = input_data)
# assign output to result
stvcs <- output_data
# summarise
stvcs |> select(starts_with("b_")) |> summary()
```

This indicates positive relationships between Private capital (the target variable) and all of the predictor variables, with weaker associations with the Intercept and Unemployment (`unemp`) and stronger ones with with Public capital (`pubC`).

It is instructive to unpick some of the model coefficients in more detail and the code below summarises variations over time through the median values of each coefficient estimate:

```{r eval = T}
stvcs |> 
  select(year, b_Intercept, b_unemp, b_pubC) |>
  group_by(year) |>
  summarise(med_b0 = median(b_Intercept),
            med_b1 = median(b_unemp),
            med_b2 = median(b_pubC))
```
It is evident that of the 2 variables and the Intercept used to model Private capital, only Public capital (`med_b2`) has large variations over time. This increase is shown visually in the figure below .   

```{r ch2stvccoefsbox, echo = T, fig.height = 4, fig.width = 7, fig.cap = "The temporal variation of the Public capital coefficient estimates over 17 years.", fig.pos = 'h'}
# inputs to plot
stvcs |> select(starts_with("b"), year) |> 
  mutate(year = "All Years") -> tmp
cols = c(c4a("tableau.red_gold", n = 17, reverse = T), "grey")
tit =expression(paste(""*beta[`Public Capital`]*""))
# plot
stvcs |> select(starts_with("b"), year) |> 
  rbind(tmp) |> 
  mutate(year = factor(year)) |> 
  ggplot(aes(y = year, x = b_pubC, fill = year)) +
  geom_boxplot(outlier.alpha = 0.1) +
  scale_fill_manual(values=cols, guide = "none") +
  theme_bw() + xlab(tit) + ylab("Time") 
```

The spatial pattern of this temporal trend can also be explored as below. This shows that the increasing intensity of the effect of Public capital on Private capital does vary spatially, with changes in the distribution of the coefficient estimates (i.e. the relationship between Public capital and Private capital) over space and time and increasing intensity in the south. 

```{r, ch2stvccoefsmap, message = F, warning = F, fig.height = 4, fig.width = 7, fig.cap = "The spatial variation of the Unemployment coefficient estimates over time."}
tit =expression(paste(""*beta[`Public Capital`]*""))
# join the data 
stvcs_sf <-
  us_data |> left_join(stvcs |> select(GEOID, year, b_Intercept, b_unemp, b_pubC), by = "GEOID")
# create the plot
stvcs_sf |>
  ggplot() + geom_sf(aes(fill = b_pubC), col = NA) +
	scale_fill_binned_c4a_div(palette="brewer.spectral", name = tit) +
  facet_wrap(~year) +
	theme_bw() + xlab("") + ylab("") + 
	theme(
	  legend.position = "inside", legend.position.inside = c(0.7, 0.1),
	  legend.direction = "horizontal",
	  legend.key.width = unit(1, "cm"),
	  strip.background = element_rect(fill="white", colour = "white"), 
	  strip.text = element_text(size = 8, margin = margin(b=4)),
	  axis.title=element_blank(),
    axis.text=element_blank(),
    axis.ticks=element_blank()) 
```

## Part 2: using `stgam` functions

### Data considerations

The code below undertakes the STVC analysis above but this time using `stgam` functions. The input data (`input_data`) is in long format (lengthened by time), with locational information in variables called `X` and `Y`, indicating the Easting and Northing coordinates from a geographic projection. 

In some situations, you may have wide format `sf` data. The process of lengthening by time is illustrated below. The first step is to widen the data and convert to a spatial format: 
```{r}
# widen
productivity_wide <- 
  productivity |>
  select(state, GEOID, year, pubC, privC, unemp, X,Y) |>
  pivot_wider(names_from = c(year), values_from = c(pubC, privC, unemp)) 
# make spatial in WGS84 format
productivity_wide_sf <- 
  productivity_wide |> st_as_sf(coords = c("X","Y"), crs = 'ESRI:102005') |>
  st_transform(4326)
# have a look
productivity_wide_sf
```

If you have data in a similar format then you can lengthen and convert to tibble format with X and Y in projected coordinates in a similar way to the code snippet below:

```{r}
# extract re-projected coordinates (ignore the warnings!)
coords <- 
  productivity_wide_sf |> 
  st_transform('ESRI:102005') |>
  st_centroid() |> 
  st_coordinates()
# remove geometry and add coordinates
productivity2 <- 
  productivity_wide_sf |>
  st_drop_geometry() |>
  mutate(X = coords[, "X"],
         Y = coords[, "Y"]) |>
  # lengthen (there is probably a neater way of doing this!)
  pivot_longer(cols = pubC_1970:unemp_1986,  names_to = c("var", "year"), names_sep = "_") |> 
  pivot_wider(names_from = var, values_from =  value)
# have a look
productivity2
```

Again to re-iterate the above code snippets are only to provide an illustration of how to lengthen, widen, convert in and out of spatial formats and re-project spatial data

```{r}
# define the input data
input_data <- 
  productivity |> 
  mutate(Intercept = 1)
```

### Optimising length scale parameters

This is done using the `opt_length_scale` function. It takes a tibble or data.frame as its input, with an Intercept term specified, and variables containing location and time of observation. The code below uses this function to determine the $\rho_{space}$ and $\rho_{time}$ parameters:

```{r eval = T, echo = F}
opt_length_scale = function(input_data, 
                            target_var = "privC",
                            vars = c("Intercept", "unemp", "pubC"), 
                            coords_x = "X",
                            coords_y = "Y",
                            STVC = FALSE,
                            time_var = NULL,...
                            ) {
  # functions
  get_XY_gcv = function(x, var) {
    f =  glue("{target_var} ~ te({coords_x},{coords_y},d=2,bs ='gp',m=list(c(3,{x})),by={var})")
    f = as.formula(f)
    gam.i = gam(f, data = input_data, method = "GCV.Cp")
    return(as.vector(gam.i$gcv.ubre))
  }  
  get_time_gcv = function(x, var) {
    f =  glue("{target_var} ~ te({time_var},d=1,bs ='gp',m=list(c(3,{x})),by={var})")
    f = as.formula(f)
    gam.i = gam(f, data = input_data, method = "GCV.Cp")
    return(as.vector(gam.i$gcv.ubre))
  }
  # space: max distance
  d_max <- 
    input_data |>
    select(all_of(coords_x), all_of(coords_y)) |>
    dist() |> as.vector() |> ceiling() |> max()
  # apply the get_XY_gcv function 
  rho_sp = NULL
  for (i in vars) {
    rho_sp.i = optimise(get_XY_gcv, c(0,d_max), var = i, maximum=FALSE)$minimum
    rho_sp = c(rho_sp, rho_sp.i)
  }
  if(STVC) {
    # time max
    t_max <- 
      input_data |>
      select(all_of(time_var)) |> 
      as.vector() |> range() |> diff() 
    # apply the get_time_gcv function 
    rho_time = NULL
    for (i in vars) {
      rho_time.i = optimise(get_time_gcv, c(0,t_max), var = i, maximum=FALSE)$minimum
      rho_time = c(rho_time, rho_time.i)
    }
    out_tab = data.frame(Vars = vars, rho_space = rho_sp, rho_time = rho_time)
    
  } else {
    out_tab = data.frame(Vars = vars, rho_space = rho_sp)
  }
  return(out_tab)
}
```

```{r}
rho_sp_st <- 
  opt_length_scale(input_data,
                   target_var = "privC",
                   vars = c("Intercept", "unemp", "pubC"),
                   coords_x = "X",
                   coords_y = "Y",
                   STVC = TRUE,
                   time_var = "year")
rho_sp_st
```


### Determining model form

Recall that for STVC models, each predictor variable can be specified in 6 ways in a GAM with TP smooths model, with 5 options for the Intercept (for SVC models this is 3 and 2 respectively). Thus for a spatial (SVC) regression  with $k$ predictor variables there are $2 \times 3^k$ potential models  and for a space-time (STVC) regression there are $5 \times 6^k$ models to evaluate. 

In the STVC problem being considered in this vignette, with $k = 2$ variables, there are therefore 180 potential space-time models to evaluate. This is done through the `evaluate_models()` function in `stgam` which uses the model GCV value (a unbiased risk estimator) to evaluate different space-time models. Note that in the code below, `ncores` is set to 2 pass CRAN diagnostic checks - you may want to specify more using `detectCores()-1` from the `parallel` package.

```{r eval = T, echo = F}
evaluate_models <- function(
    input_data = input_data,
    target_var = "privC",
    vars = c("unemp", "pubC"),
    coords_x = "X",
    coords_y = "Y",
    STVC = FALSE,
    time_var = NULL,
    rho_space_vec = NULL,
    rho_time_vec = NULL,
    ncores = 2)
{
  # function to get model intercept terms
  get_form_intercept = function(index,rho_space = NULL, rho_time = NULL, bs = 'gp') {
    c("",
      glue("+te({coords_x},{coords_y},d=2,m=list(c(3,{rho_space})),bs='{bs}',by=Intercept)"),
      glue("+te({time_var},d=1,m=list(c(3,{rho_time})),bs='{bs}',by=Intercept)"),
      glue("+te({coords_x},{coords_y},d=2,m=list(c(3,{rho_space})),bs='{bs}',by=Intercept) + te({time_var},d=1,m=list(c(3,{rho_time})),bs='{bs}',by=Intercept)"),
      glue("+te({coords_x},{coords_y},{time_var},d=c(2,1),m=list(c(3,{rho_space}),c(3,{rho_time})),bs='{bs}',by=Intercept)"))[index]
  }

  # function to get model predictor terms
  get_form_covariate = function(varname, index, rho_space = NULL, rho_time = NULL, bs = "gp") {
     c("",
      glue("+ {varname}"),
      glue("+te({coords_x},{coords_y},d=2,m=list(c(3,{rho_space})),bs='{bs}',by={varname})"),
      glue("+te({time_var},d=1,m=list(c(3,{rho_time})),bs='{bs}',by={varname})"),
      glue("+te({coords_x},{coords_y},d=2,m=list(c(3,{rho_space})),bs='{bs}',by={varname}) + te({time_var},d=1,m=list(c(3,{rho_time})),bs='{bs}',by={varname})"),
      glue("+te({coords_x},{coords_y},{time_var},d=c(2,1),m=list(c(3,{rho_space}),c(3,{rho_time})),bs='{bs}',by={varname})"))[index]
  }

  # function to make SVC index grid
  make_svc_index_grid = function(vars) {
    expression_x = "expand.grid(Intercept = 1:2"
    for (i in vars) {
      expression_x = paste0(expression_x, ",", i, " = 1:3")
    }
    expression_x = paste0(expression_x, ")")
    eval(parse(text = expression_x))
  }

  # function to make STVC index grid
  make_stvc_index_grid = function(vars) {
    expression_x = "expand.grid(Intercept = 1:5"
    for (i in vars) {
      expression_x = paste0(expression_x, ",", i, " = 1:6")
    }
    expression_x = paste0(expression_x, ")")
    eval(parse(text = expression_x))
  }

  # function to make GAM model formula
  get_formula = function(indices, rho_space_vec, rho_time_vec) {
    form.i = glue("{target_var}~Intercept-1")
    form.i = paste0(form.i, get_form_intercept(indices[1],rho_space_vec[1], rho_time_vec[1]))
    for (j in 1:length(vars)) {
      varname = vars[j]
      form.i = paste0(form.i, get_form_covariate(varname, indices[j+1], rho_space_vec[j+1], rho_time_vec[j+1]))
    }
    return(formula(form.i))
  }

  # function to generate GAM TP smooth model formula
  evaluate_gam = function(i, terms_grid, input_data, rho_space_vec, rho_time_vec = NULL, ...) {
    indices = unlist(terms_grid[i, ])
    f <- get_formula(indices, rho_space_vec, rho_time_vec)
    input_data <- mutate(input_data, Intercept = 1)
    m = gam(f, data = input_data, method = "GCV.Cp")
    gcv = m$gcv.ubre
    index = data.frame(terms_grid[i, ])
    #f <- get_formula(indices, round(rho_space_vec,3), round(rho_time_vec, 3))
    f = paste0(target_var, " ~ ", as.character(f)[3])
    return(data.frame(index, gcv,f))
  }
 
  # 1. make the terms grid
  if (!STVC) {
    terms_grid = make_svc_index_grid(vars)
  } else {
    terms_grid = make_stvc_index_grid(vars)
  }

  # 2. evaluate each model
  # a) in a for loop if n <= 30
  if(STVC) {
    if (nrow(terms_grid) <= 30) {
      vc_res_gam <- NULL
      for (i in 1:nrow(terms_grid)) {
        res.i = evaluate_gam(i, terms_grid, input_data, rho_space_vec, rho_time_vec)
        vc_res_gam = rbind(vc_res_gam, res.i)
      }
    } else {
      # b) in parallel if n > 30
      #t1 = Sys.time()
      cl = makeCluster(ncores)
      registerDoParallel(cl)
      vc_res_gam <- foreach(i = 1:nrow(terms_grid), .combine = "rbind",
                            .packages = c("glue", "mgcv", "purrr", "dplyr")) %dopar%
        {evaluate_gam(i, terms_grid, input_data, rho_space_vec, rho_time_vec,
                      target_var, vars, coords_x, coords_y, time_var)
        }
      stopCluster(cl)
      # Sys.time() - t1 # 15 minutes
    }
  } else {
    if (nrow(terms_grid) <= 30) {
      vc_res_gam <- NULL
      for (i in 1:nrow(terms_grid)) {
        res.i = evaluate_gam(i, terms_grid, input_data, rho_space_vec)
        vc_res_gam = rbind(vc_res_gam, res.i)
      }
    } else {
      # b) in parallel if n > 30
      #t1 = Sys.time()
      cl = makeCluster(ncores)
      registerDoParallel(cl)
      vc_res_gam <- foreach(i = 1:nrow(terms_grid), .combine = "rbind",
                            .packages = c("glue", "mgcv", "purrr", "dplyr")) %dopar%
        {evaluate_gam(i, terms_grid, input_data, rho_space_vec,
                      target_var, vars, coords_x, coords_y)
        }
      stopCluster(cl)
      # Sys.time() - t1 # 15 minutes
    }
  }  
  return(vc_res_gam)
}
```

```{r do_final_stvc}
stvc_mods = evaluate_models(
  input_data = input_data, 
  target_var = "privC", 
  vars = c("unemp", "pubC"),
  coords_x = "X", 
  coords_y = "Y", 
  STVC = TRUE, 
  time_var = "year", 
  rho_space_vec = round(rho_sp_st$rho_space,1),
  rho_time_vec = round(rho_sp_st$rho_time,2),
  ncores = 2)
```

The best 10  models can be extracted (i.e. those with the lowest GCV score) using the `gam_model_score()` function. Interestingly the top ranked models are dominated by a single space-time TP smooth for `pubC` and temporal TP smooths of fixed parametric terms for the Intercept and `unemp`. 

```{r echo = F, eval = T}
gam_model_scores <- function(res_tab, n = 10) {
  Rank = NULL
  GVC = NULL
  nm <- names(res_tab)
  len = length(nm)
  mod_comp <- rename(tibble(res_tab), GCV = gcv) |> arrange(GCV)
  int_terms <- function(x) c("Fixed", "te_S", "te_T", "te_T + te_S", "te_ST")[x]
  var_terms <- function(x) c("---", "Fixed", "te_S", "te_T", "te_T + te_S", "te_ST")[x]
  out_tab <- 
    relocate(
      mutate(
        mutate(
          mutate(
            slice_head(mod_comp, n = n), 
            across(nm[2]:nm[len - 2], var_terms)), 
          across(nm[1]:nm[1], int_terms)), 
        Rank = 1:n()), 
      Rank)
    return(out_tab)
}
```
```{r}
mod_comp <- gam_model_scores(stvc_mods)
# have a look
mod_comp |> select(-f) 
```

### The final model 

A final model for use in analysis can be specified in the form of the top ranked model above. This included the Intercept and `unemp` in temporal TP smooths and `pubC` in space-time TP smooth. Recall that the marginal bases in the TP smooth are defined as 2 dimensional for location and as 1 dimensional for time, with a third-order smooth basis (i.e. a Mat√©rn correlation function with $\kappa = 1.5$) and length scales (penalties) in both cases. 

First the formula is extracted and can be inspected:

```{r}
f = as.formula(mod_comp$f[1])
f
```

Then this is put into a `mgcv` GAM model, and checked for over-fitting:
```{r}
m = gam(f, data = input_data)
k.check(m)
```


### The varying coefficients

The space-time varying coefficient estimates can be extracted using the `calculate_vcs()` function in `stgam`. This returns a data table in `tibble` format which is linked to the spatial data in the code snippet below:

```{r}
vcs = calculate_vcs(input_data, model = m, terms = c("Intercept","unemp", "pubC"))
# add the residuals
vcs$residuals <- m$residuals
# make spatial and move the geometry to the end
vcs_sf = 
  left_join(us_data |> select(GEOID), vcs) |> 
  relocate(geometry, .after = last_col())
```

The varying coefficient estimates can be summarised (and mapped) as before:
```{r eval = F}
## not run ##
# summary over space-time
vcs |> select(starts_with("b_")) |> summary()
# summary over time
vcs |> 
  select(year, b_Intercept, b_unemp, b_pubC) |>
  group_by(year) |>
  summarise(med_b0 = median(b_Intercept),
            med_b1 = median(b_unemp),
            med_b2 = median(b_pubC))
# etc - see ggplot routines above!
```


## Summary

The rationale for using GAMs with TP smooths and GP bases for spatially varying coefficient (SVC) or temporally varying coefficient (TVC) models is as follows: 

- GAMs with smooths (splines) capture non-linear relationships between the response variable and covariates.
- Smooths generate a varying coefficient model when they are parameterised with more than one variable. 
- This is readily extending to the temporal and / or spatial dimensions to generate SVCs and STVCs.
- Different smooths are available, but i) Tensor Products smooths can be used combine space and time (because of the specification of individual length scale parameters for $\rho_{space}$ and $\rho_{time}$ in the smooth), and ii) specifying TP smooths with GP bases reflects Tobler's First Law of Geography (spatial autocorrelation and decay).
- This can be extended to the temporal case on the assumption of temporal decay (similarity decreases over time).
- The optimised $\rho_{space}$ and $\rho_{time}$ values provide an explicit measure of the *scales* of interaction between each target variable and the predictor variable (i.e. a multi-scale space-time analysis).
- GAMs are robust, have a rich theoretical background and been subject to much development.

The workflow suggested in this vignette and in the `stgam` package is to first optimise the length scale parameters. This provides a direct indication of the scales of the individual target-to-predictor variable space-time (i.e. multiscale) interactions through the $\rho_{space}$ and $\rho_{time}$ parameters. Second, to then determine the most appropriate model form (both steps evaluated by minimising a Generalized Cross-Validation (GCV), an un-biased risk estimator the model recommended for evaluating `mgcv` GAMs [@wood2017generalized]). This avoids making potentially unreasonable assumptions about how space and time interact in spatially and temporally varying coefficient (STVC) models. 


## References



